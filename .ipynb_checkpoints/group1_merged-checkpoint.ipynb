{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capstone\n",
    "#Group 1\n",
    "import webbrowser\n",
    "import requests\n",
    "import flask\n",
    "import sqlite3\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "class data:\n",
    "    def __init__(self):\n",
    "        #create table if not exists Jobs (jobTitle text, passWord text, userType text)\n",
    "        #info given by user\n",
    "        self.location=\"none\"\n",
    "        self.jobType=\"no type\"\n",
    "        self.skills=[]\n",
    "        self.exp=\"none\"\n",
    "        self.edu=\"none\"\n",
    "\n",
    "        self.cnt=1\n",
    "        \n",
    "        #info given by api\n",
    "        self.age=[]#how old is the job listing\n",
    "        self.jobLst=[]\n",
    "        self.jobDist=[]\n",
    "        self.jobLocation=\"none\"\n",
    "        self.company=\"none\"\n",
    "        self.listing=[]\n",
    "        \n",
    "        # keywords used in skill identification\n",
    "        self.keyWordSkills = ['python', 'java', 'c++', 'sql', 'manage', 'javascript', 'linux', 'team', 'problem solving', 'front end', 'back end', 'html', 'css','json', 'xml','api', 'linux', 'nodejs', 'c#', 'spark', 'sas', 'matlab', 'excel', 'spark', 'hadoop', 'azure', 'spss', 'git', 'aws']\n",
    "        self.keyWordEdu = ['masters', 'bachelors', \"master's\", \"bachelor's\", 'phd', 'undergrad', 'graduate', 'undergraduate', 'ged', \"graduate's\", \"undergraduate's\", \"associate's\", 'doctoral']\n",
    "        self.aiKeys = ['ai', 'a.i.', 'artificial intelligence', 'artificial']\n",
    "        self.dlKeys= ['deep learning', 'neural networks', 'big data', 'deep', 'statistics']\n",
    "        self.mlKeys = ['data mining', 'machine learning', 'cnn', 'rbm', \n",
    "          'machine', 'natural language', 'regression', 'fault diagnosis', 'intrusion detection']\n",
    "        self.seKeys = ['software engineer', 'software development','code']\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.exp\n",
    "    \n",
    "    # TODO: remove function?\n",
    "    # sends the data allocated to the database\n",
    "    def testing(self):\n",
    "        conn = sqlite3.connect(\"Flask_Jade_Sample/TestFlaskJadeWeb/Users.db\")\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # create table if not exists\n",
    "        # columns used for the primary key implicitly cannot be null\n",
    "        # columns skills and education are comma separated string representations of lists\n",
    "        table_query = \"\"\"create table if not exists JOBS\n",
    "                            (location text, company text, datePosted text, postUrl text, \n",
    "                            jobType text, jobTitle text, jobDes text, jobApp text, salary text,\n",
    "                            skills text, category text, education text,\n",
    "                            PRIMARY KEY (company, jobTitle, location))\"\"\"\n",
    "        cursor.execute(table_query)\n",
    "        conn.commit()\n",
    "        \n",
    "        for i in range(len(self.jobLst)):\n",
    "            insert_query = \"\"\"insert or ignore into JOBS (location, company, datePosted, postUrl, \n",
    "                                                jobType, jobTitle, jobDes, jobApp, salary, skills,\n",
    "                                                category, education) \n",
    "                                    VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\"\"\"\n",
    "            \n",
    "            #print(self.jobLst[i]['Skills'])\n",
    "            data_tuples = (self.jobLst[i]['Location'], self.jobLst[i]['Company'], \n",
    "                           self.jobLst[i]['Time-Posted'], self.jobLst[i]['Page-Addr'], \n",
    "                           self.jobLst[i]['Contract-Type'], self.jobLst[i]['Title'], \n",
    "                           self.jobLst[i]['Desc'], self.jobLst[i]['Apply-To'], self.jobLst[i]['Salary'],\n",
    "                           lst_to_str(self.jobLst[i]['Skills']), self.jobLst[i]['Category'], lst_to_str(self.jobLst[i]['Education']))\n",
    "\n",
    "            cursor.execute(insert_query, data_tuples)\n",
    "\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "    # searches backend for whatever search term\n",
    "    def searchJobs(self, term):\n",
    "        results = []\n",
    "        \n",
    "        conn = sqlite3.connect(\"Flask_Jade_Sample/TestFlaskJadeWeb/Users.db\")\n",
    "        cursor = conn.cursor()\n",
    "        select_query = \"\"\"select * from JOBS \"\"\" # change JOBS to whatever table you want to see\n",
    "        \n",
    "        cursor.execute(select_query)\n",
    "        records = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        for i in range(len(records)):\n",
    "            if term in records[i]:\n",
    "                results.append(records[i])\n",
    "                #return records[i] # TODO: remove to return multiple results\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    # returns tuple list of all records in jobs table sorted by the job title in ascending order\n",
    "    def getAllJobs(self):\n",
    "        conn = sqlite3.connect(\"Flask_Jade_Sample/TestFlaskJadeWeb/Users.db\")\n",
    "        conn.row_factory = sqlite3.Row\n",
    "        cursor = conn.cursor()\n",
    "        select_query = \"\"\"select * from Jobs order by jobTitle ASC\"\"\"\n",
    "        cursor.execute(select_query)\n",
    "        records = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return [dict(row) for row in records]\n",
    "\n",
    "    # returns tuple list of jobs starting from the offset and getting as many as amount\n",
    "    # 0 based indexing means offset at 1 will start at second index\n",
    "    def getNJobs(self, offset, amt):\n",
    "        conn = sqlite3.connect(\"Flask_Jade_Sample/TestFlaskJadeWeb/Users.db\")\n",
    "        conn.row_factory = sqlite3.Row\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        select_query = \"\"\"select * from Jobs order by jobTitle ASC\"\"\"\n",
    "        cursor.execute(select_query)\n",
    "        records = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return [[records.index(row), dict(row)] for row in records[offset:offset+amt]]\n",
    "    \n",
    "    # returns a single job as a dictionary based on its index when sorting by title\n",
    "    def getNthJob(self, n):\n",
    "        conn = sqlite3.connect(\"Flask_Jade_Sample/TestFlaskJadeWeb/Users.db\")\n",
    "        conn.row_factory = sqlite3.Row\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        select_query = \"\"\"select * from Jobs order by jobTitle ASC\"\"\"\n",
    "        cursor.execute(select_query)\n",
    "        records = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return [dict(row) for row in records][n]\n",
    "    \n",
    "    # deletes the jobs table entirely\n",
    "    def destroyJobs(self):\n",
    "        conn = sqlite3.connect(\"Flask_Jade_Sample/TestFlaskJadeWeb/Users.db\")\n",
    "        cursor = conn.cursor()\n",
    "        table_query = 'DROP TABLE JOBS'\n",
    "        cursor.execute(table_query)\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "    \n",
    "###CAN CHANGE TO RUN LINKS THROUGH METHODS (have links as method parameters)\n",
    "\n",
    "    #returns list of links of positions without location specification \n",
    "    def onlylinks(self):\n",
    "        linklist = []\n",
    "        url = 'https://jobs.github.com/positions'\n",
    "        source = requests.get(url).text\n",
    "        soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "        for link in soup.find_all('a'):\n",
    "            links = link.get('href')\n",
    "            if \"http\" in links and '/positions/' in links:\n",
    "               #print (links)\n",
    "                linklist.append(link['href'])\n",
    "        return linklist\n",
    "            \n",
    "    # searches multiple pages and the given location\n",
    "    # returns list of all jobs found\n",
    "    # also returns links and other info\n",
    "    def hubJobs(self, jobX, loc, numPages):\n",
    "        print('entering github jobs')\n",
    "        multPageLocJobsWL = [] # will be populated with job dictionaries\n",
    "        for i in range (0, numPages): # iterating over pages until numPages\n",
    "            loc = '' if loc == '' else ('&location=' + loc.replace(' ', '+'))\n",
    "            jobX = '' if jobX == '' else ('&description=' + jobX.replace(' ', '+'))\n",
    "            finalURL = 'https://jobs.github.com/positions?utf8=âœ“' + loc + '&page=' + str(i) + jobX\n",
    "            #finalURL = url + loc # including location in filters\n",
    "            \n",
    "            source = requests.get(finalURL).text\n",
    "            soup = BeautifulSoup(source, 'lxml')\n",
    "            \n",
    "            for job in soup.find_all('tr', {'class':'job'} ): # iterating over individual job data\n",
    "                tmp = job.text.strip().split('\\n')\n",
    "                jb = {}\n",
    "                for x in range (0, len(tmp)):\n",
    "                    y = tmp[x].strip()\n",
    "                    if len(y) > 1 and not \"\\t\" in y:\n",
    "                        if x == 0:\n",
    "                            jb['Title'] = y\n",
    "                        \n",
    "                        elif x == 2:\n",
    "                            jb['Company'] = y\n",
    "                        \n",
    "                        elif x == 4:\n",
    "                            jb['Contract-Type'] = y\n",
    "                        \n",
    "                        elif x == 7:\n",
    "                            jb['Location'] = y\n",
    "                        \n",
    "                        elif x == 8:\n",
    "                            jb['Time-Posted'] = y\n",
    "                        \n",
    "                jb['Salary'] = 'N/A' # salary not listed on github\n",
    "                jobMeta = self.getPageMeta (job.find('td', {'class':'title'}).find('h4').find('a')['href']) # gets job info (applyto link and skills)\n",
    "                jb['Apply-To'] = jobMeta[0]\n",
    "                jb['Skills'] = jobMeta[1]\n",
    "                jb['Desc'] = jobMeta[2]\n",
    "                jb['Page-Addr'] = jobMeta[3]\n",
    "                jb['Education'] = jobMeta[4]\n",
    "                jb['Category'] = jobMeta[5]\n",
    "                \n",
    "                multPageLocJobsWL.append(jb)\n",
    "                \n",
    "        print('github jobs length ', len(multPageLocJobsWL))\n",
    "        return multPageLocJobsWL\n",
    "            \n",
    "    # TODO\n",
    "    #going to be a list of dictionaries ***TBA\n",
    "    #for now prints out several dictionaries for each job post on the page (50)\n",
    "    def create(self, job = '', loc = '', numPages = 1):\n",
    "        self.jobLst = self.hubJobs (job, loc, numPages) + self.indeedJobs(job, loc, numPages)\n",
    "        #self.jobLst += self.indeedJobs(job, loc, numPages)\n",
    "        self.testing()\n",
    "        print(\"create done\")\n",
    "\n",
    "    # returns 4 pieces of the specified page\n",
    "    #   [string:applyTo_link, list:skills, string:description, string:page_url, list:education]\n",
    "    def getPageMeta(self, url):\n",
    "        newSrc = requests.get(url).text\n",
    "        newSoup = BeautifulSoup(newSrc, 'lxml')\n",
    "        \n",
    "        # apply-to link\n",
    "        jobLink = ''\n",
    "        try:\n",
    "            jobLink = newSoup.find('div', {'class':'highlighted'}).find('a')['href']\n",
    "        except:\n",
    "            jobLink = url\n",
    "        \n",
    "        foundSkillsList = []\n",
    "        foundEduList = []\n",
    "\n",
    "        # parsing summary\n",
    "        summary = newSoup.find('div', class_=['column main', 'jobsearch-jobDescriptionText'])\n",
    "        sumtext = summary.text if not summary == None else 'N/A'\n",
    "        sumtext1 = sumtext.lower()\n",
    "        sumList = sumtext1.split()\n",
    "        \n",
    "        # education\n",
    "        for i in self.keyWordEdu:\n",
    "            if i in sumList and not i in foundEduList:\n",
    "                foundEduList.append(i)\n",
    "                \n",
    "        # skills\n",
    "        for i in self.keyWordSkills:\n",
    "            if i in sumList and not i in foundSkillsList:\n",
    "                foundSkillsList.append(i)\n",
    "        \n",
    "        # primitive text classification\n",
    "        # sums up occurunces of keywords and then appends the category tag associated with the highest count\n",
    "        cat = ''\n",
    "        aiCNT = 0\n",
    "        dlCNT = 0\n",
    "        mlCNT = 0\n",
    "        seCNT = 0\n",
    "        otherCNT = 0\n",
    "        for x in sumList:\n",
    "            if x in self.aiKeys:\n",
    "                aiCNT += 1\n",
    "                continue\n",
    "            elif x in self.dlKeys:\n",
    "                dlCNT += 1\n",
    "                continue\n",
    "            elif x in self.mlKeys:\n",
    "                mlCNT += 1\n",
    "                continue\n",
    "            elif x in self.seKeys:\n",
    "                seCNT += 1\n",
    "                continue\n",
    "                \n",
    "        mx = max(aiCNT, dlCNT, mlCNT, seCNT, otherCNT)\n",
    "        if aiCNT == mx:\n",
    "            cat = 'Artificial Intelligence'\n",
    "        elif dlCNT == mx:\n",
    "            cat = 'Deep Learning'\n",
    "        elif mlCNT == mx:\n",
    "            cat = 'Machine Learning'\n",
    "        elif seCNT == mx:\n",
    "            cat = 'Software Engineer'\n",
    "        elif otherCNT == mx: # consider difference of counts?\n",
    "            cat = 'Other'\n",
    "        \n",
    "        return [jobLink, foundSkillsList, sumtext, url, foundEduList, cat]\n",
    "    \n",
    "    ### Indeed ###\n",
    "    \n",
    "    # gets a list of dictionaries limited by the input parameters through indeed\n",
    "    def indeedJobs(self, job, location, maxPages):\n",
    "        print('entering indeed jobs')\n",
    "        baseLink = 'https://www.indeed.com/'\n",
    "        webAddr = baseLink + ('' if job == '' else 'jobs?q=' + job.replace (' ', '+'))\n",
    "        webAddr = webAddr + ('' if location == '' else '&l=' + location) + '&start=0'\n",
    "\n",
    "        jbPages = []\n",
    "        for x in range(0, maxPages):\n",
    "            link=webAddr.replace(webAddr[-1], str(x))\n",
    "            #jbPages.append(self.getDictNew(link))\n",
    "            source = requests.get(link).text\n",
    "            soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "            for div in soup.find_all ('div', class_='row', attrs={'class':'row'}):\n",
    "                linkElem = div.find('div', class_='title').a\n",
    "                title = linkElem.get('title') # TITLE\n",
    "                link = \"https://www.indeed.com\" + linkElem.get('href')   # LINK\n",
    "\n",
    "                payRAW = div.find(name='span', class_=['salaryText', 'sjcl', 'salary'])\n",
    "                pay = payRAW.text.replace('\\n', '') if not payRAW == None else 'N/A' # salary\n",
    "\n",
    "                co = div.find(name='span', class_=['company', 'result-link-source']).text.strip() # company\n",
    "                loc = div.find(['div', 'span'], attrs={'class': 'location'}).text # location\n",
    "                date = div.find('span', attrs={'class': 'date'}).text # date\n",
    "\n",
    "                jobMeta = self.getPageMeta(link)\n",
    "                newDict = {\n",
    "                    'Company':co, 'Location':loc, 'Title':title, \n",
    "                    'Time-Posted':date, 'Salary':pay, 'Link':link,\n",
    "                    'Contract-Type':'N/A'\n",
    "                }\n",
    "                newDict['Apply-To'] = jobMeta[0]\n",
    "                newDict['Skills'] = jobMeta[1]\n",
    "                newDict['Desc'] = jobMeta[2]\n",
    "                newDict['Page-Addr'] = jobMeta[3]\n",
    "                newDict['Education'] = jobMeta[4]\n",
    "                newDict['Category'] = jobMeta[5]\n",
    "\n",
    "                jbPages.append(newDict)\n",
    "        \n",
    "        print('indeed jobs length ', len(jbPages))\n",
    "        return jbPages\n",
    "    \n",
    "    # TODO: delete this\n",
    "    def getDictNew(self, url):\n",
    "        source = requests.get(url).text\n",
    "        soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "        tmpLst = []\n",
    "        for div in soup.find_all ('div', class_='row', attrs={'class':'row'}):\n",
    "            linkElem = div.find('div', class_='title').a\n",
    "            title = linkElem.get('title') # TITLE\n",
    "            link = \"https://www.indeed.com\" + linkElem.get('href')   # LINK\n",
    "\n",
    "            payRAW = div.find(name='span', class_=['salaryText', 'sjcl', 'salary'])\n",
    "            pay = payRAW.text.replace('\\n', '') if not payRAW == None else 'N/A' # salary\n",
    "\n",
    "            co = div.find(name='span', class_=['company', 'result-link-source']).text.strip() # company\n",
    "            loc = div.find(['div', 'span'], attrs={'class': 'location'}).text # location\n",
    "            date = div.find('span', attrs={'class': 'date'}).text # date\n",
    "            \n",
    "            jobMeta = self.getPageMeta(link)\n",
    "            newDict = {\n",
    "                'Company':co, 'Location':loc, 'Title':title, 'Time-Posted':date, 'Salary':pay, 'Link':link\n",
    "            }\n",
    "            newDict['Apply-To'] = jobMeta[0]\n",
    "            newDict['Skills'] = jobMeta[1]\n",
    "            newDict['Desc'] = jobMeta[2]\n",
    "            newDict['Page-Addr'] = jobMeta[3]\n",
    "            newDict['Education'] = jobMeta[4]\n",
    "            \n",
    "            tmpLst.append(newDict)\n",
    "        return tmpLst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the given list to a string\n",
    "def lst_to_str(lst):\n",
    "    return ','.join(lst)\n",
    "\n",
    "# converts the given string to a list\n",
    "def str_to_lst(stri):\n",
    "    return stri.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGES\n",
    "# + updated to match commit 701d273a962d13486985e1c8852af6733eab4f77\n",
    "# + updated to match commit fa30ef6af9ef9659831d2fbc5e526715340fc29e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('USERS',), ('JOBS',)]\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(\"Flask_Jade_Sample/TestFlaskJadeWeb/Users.db\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cursor.fetchall())\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entering github jobs\n",
      "github jobs length  121\n",
      "entering indeed jobs\n",
      "indeed jobs length  152\n",
      "create done\n"
     ]
    }
   ],
   "source": [
    "d.create('science', '', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.destroyJobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.getAllJobs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
